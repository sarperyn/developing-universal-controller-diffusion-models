{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn1 = torch.load('attention_maps/input-1.1.transformer_blocks.0.attn1.to_out.0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(attn1[0].cpu().detach(), cmap='hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.data_utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import abc\n",
    "import torch.nn.functional as F\n",
    "import open_clip\n",
    "import numpy as np\n",
    "import einops\n",
    "\n",
    "\n",
    "\n",
    "LOW_RESOURCE = False \n",
    "NUM_DIFFUSION_STEPS = 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "\n",
    "tokenizer = open_clip.SimpleTokenizer()\n",
    "\n",
    "\n",
    "def get_word_inds(text, word_place, tokenize):\n",
    "    split_text = text.split(\" \")\n",
    "    if type(word_place) is str:\n",
    "        word_place = [i for i, word in enumerate(split_text) if word_place == word]\n",
    "        print(word_place)\n",
    "    elif type(word_place) is int:\n",
    "        word_place = [word_place]\n",
    "    out = []\n",
    "    if len(word_place) > 0:\n",
    "        words_encode = [tokenizer.decode([item]).strip(\"#\") for item in tokenizer.encode(text)] # TODO 1st letter should not be removed\n",
    "        print(words_encode)\n",
    "        cur_len, ptr = 0, 0\n",
    "\n",
    "        for i in range(len(words_encode)):\n",
    "            cur_len += len(words_encode[i])\n",
    "            if ptr in word_place:\n",
    "                out.append(i + 1)\n",
    "            if cur_len >= len(split_text[ptr]):\n",
    "                ptr += 1\n",
    "                cur_len = 0\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8]\n",
      "['a ', 'painting ', 'of ', 'a ', 'lion ', 'eating ', 'a ', 'burger ', 'painting ']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 9])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open_clip\n",
    "key = 'painting'\n",
    "prompts = [\"A painting of a lion eating a burger painting\"]\n",
    "inds = get_word_inds(prompts[0], key, tokenizer)\n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
